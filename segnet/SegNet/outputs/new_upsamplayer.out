+ date
Mon Aug  1 17:32:14 EDT 2016
+ export MODULEPATH=/home/welling/git/bridges_modules::/opt/modulefiles
+ MODULEPATH=/home/welling/git/bridges_modules::/opt/modulefiles
+ module load cuda/7.5
++ /usr/bin/modulecmd bash load cuda/7.5
+ eval CUDA_ROOT=/opt/packages/cuda/7.5 ';export' 'CUDA_ROOT;LD_LIBRARY_PATH=/opt/packages/cuda/7.5/lib64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4:/opt/intel/debugger_2016/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64_lin/gcc4.4:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64_lin' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5' ';export' 'LOADEDMODULES;PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin' ';export' 'PATH;_LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5' ';export' '_LMFILES_;'
++ CUDA_ROOT=/opt/packages/cuda/7.5
++ export CUDA_ROOT
++ LD_LIBRARY_PATH=/opt/packages/cuda/7.5/lib64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4:/opt/intel/debugger_2016/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64_lin/gcc4.4:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64_lin
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5
++ export LOADEDMODULES
++ PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ export PATH
++ _LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5
++ export _LMFILES_
+ module load tensorflow/0.9.0
++ /usr/bin/modulecmd bash load tensorflow/0.9.0
+ eval LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5:tensorflow/0.9.0 ';export' 'LOADEDMODULES;TENSORFLOW_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv' ';export' 'TENSORFLOW_ENV;_LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5:/home/welling/git/bridges_modules/tensorflow/0.9.0' ';export' '_LMFILES_;'
++ LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5:tensorflow/0.9.0
++ export LOADEDMODULES
++ TENSORFLOW_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ export TENSORFLOW_ENV
++ _LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5:/home/welling/git/bridges_modules/tensorflow/0.9.0
++ export _LMFILES_
+ source /opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv/bin/activate
++ deactivate nondestructive
++ unset -f pydoc
++ '[' -z '' ']'
++ '[' -z '' ']'
++ '[' -n /bin/bash ']'
++ hash -r
++ '[' -z '' ']'
++ unset VIRTUAL_ENV
++ '[' '!' nondestructive = nondestructive ']'
++ VIRTUAL_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ export VIRTUAL_ENV
++ _OLD_VIRTUAL_PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ PATH=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv/bin:/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ export PATH
++ '[' -z '' ']'
++ '[' -z '' ']'
++ _OLD_VIRTUAL_PS1=
++ '[' x '!=' x ']'
+++ basename /opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ PS1='(TensorFlowEnv) '
++ export PS1
++ alias pydoc
++ '[' -n /bin/bash ']'
++ hash -r
+ python segnet_train.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:8b:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:8b:00.0)
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3321 get requests, put_count=3295 evicted_count=1000 eviction_rate=0.30349 and unsatisfied allocation rate=0.339055
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
cnn inputs
batch of images (4, 360, 480, 3)
batch of labels (4, 360, 480, 1)
covn1 (4, 360, 480, 64)
pool1 (4, 180, 240, 64)
covn2 (4, 180, 240, 64)
pool2 (4, 90, 120, 64)
covn3 (4, 90, 120, 64)
pool3 (4, 45, 60, 64)
covn4 (4, 45, 60, 64)
pool4 (4, 23, 30, 64)
up4 (4, 45, 60, 64)
de_covn4 (4, 45, 60, 64)
up3 (4, 90, 120, 64)
de_covn3 (4, 90, 120, 64)
up2 (4, 180, 240, 64)
de_covn2 (4, 180, 240, 64)
up1 (4, 360, 480, 64)
de_covn1 (4, 360, 480, 64)
start a session
start on training
num_examples_per_step,duration 4 4.53588294983
2016-08-01 17:32:26.205320: step 0, loss = 2.34 (0.9 examples/sec; 4.536 sec/batch)
num_examples_per_step,duration 4 0.722635030746
2016-08-01 17:32:57.392493: step 20, loss = 6.73 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.725326061249
2016-08-01 17:33:16.973044: step 40, loss = 6.37 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725042104721
2016-08-01 17:33:36.487811: step 60, loss = 8.05 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725352048874
2016-08-01 17:33:56.003438: step 80, loss = 2.53 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724045991898
2016-08-01 17:34:15.627283: step 100, loss = 2.24 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.72305893898
2016-08-01 17:34:35.200635: step 120, loss = 4.40 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.729984998703
2016-08-01 17:34:54.738919: step 140, loss = 4.63 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73016500473
2016-08-01 17:35:14.290777: step 160, loss = 3.98 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729951858521
2016-08-01 17:35:33.837440: step 180, loss = 2.60 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728416204453
2016-08-01 17:35:53.396747: step 200, loss = 1.56 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728708028793
2016-08-01 17:36:13.040206: step 220, loss = 1.52 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730432987213
2016-08-01 17:36:32.849947: step 240, loss = 2.01 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.725692987442
2016-08-01 17:36:52.539224: step 260, loss = 1.33 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729305028915
2016-08-01 17:37:12.187021: step 280, loss = 1.09 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730621099472
2016-08-01 17:37:31.822472: step 300, loss = 1.76 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.737446069717
2016-08-01 17:37:51.473884: step 320, loss = 3.38 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730857133865
2016-08-01 17:38:11.125569: step 340, loss = 1.43 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732351064682
2016-08-01 17:38:30.774612: step 360, loss = 2.59 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731498003006
2016-08-01 17:38:50.429784: step 380, loss = 1.59 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730172872543
2016-08-01 17:39:10.123193: step 400, loss = 1.23 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731462955475
2016-08-01 17:39:29.825359: step 420, loss = 1.60 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.741272211075
2016-08-01 17:39:49.635716: step 440, loss = 1.18 (5.4 examples/sec; 0.741 sec/batch)
num_examples_per_step,duration 4 0.731942892075
2016-08-01 17:40:09.347413: step 460, loss = 1.74 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729617118835
2016-08-01 17:40:29.140704: step 480, loss = 1.26 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733906030655
2016-08-01 17:40:48.942144: step 500, loss = 1.72 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731354951859
2016-08-01 17:41:08.762251: step 520, loss = 2.32 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732134819031
2016-08-01 17:41:28.509180: step 540, loss = 1.40 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.734543085098
2016-08-01 17:41:48.213467: step 560, loss = 1.69 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.731297969818
2016-08-01 17:42:07.951715: step 580, loss = 1.13 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730803012848
2016-08-01 17:42:27.800007: step 600, loss = 1.19 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728644132614
2016-08-01 17:42:47.507700: step 620, loss = 1.40 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.73037815094
2016-08-01 17:43:07.258033: step 640, loss = 2.01 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734027147293
2016-08-01 17:43:26.945349: step 660, loss = 2.53 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730468988419
2016-08-01 17:43:46.783258: step 680, loss = 1.16 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734224081039
2016-08-01 17:44:06.509083: step 700, loss = 1.30 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.743190050125
2016-08-01 17:44:27.678930: step 720, loss = 2.01 (5.4 examples/sec; 0.743 sec/batch)
num_examples_per_step,duration 4 0.733488082886
2016-08-01 17:44:47.442699: step 740, loss = 1.38 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731737136841
2016-08-01 17:45:07.305942: step 760, loss = 1.38 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732821941376
2016-08-01 17:45:27.044941: step 780, loss = 2.27 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.737185001373
2016-08-01 17:45:46.822590: step 800, loss = 2.78 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.7327709198
2016-08-01 17:46:06.674560: step 820, loss = 1.97 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.734204053879
2016-08-01 17:46:26.387840: step 840, loss = 1.54 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730752944946
2016-08-01 17:46:46.169651: step 860, loss = 1.17 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.739683151245
2016-08-01 17:47:05.947925: step 880, loss = 1.84 (5.4 examples/sec; 0.740 sec/batch)
num_examples_per_step,duration 4 0.736952066422
2016-08-01 17:47:25.700808: step 900, loss = 1.95 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730608224869
2016-08-01 17:47:45.613966: step 920, loss = 2.38 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733216047287
2016-08-01 17:48:05.321050: step 940, loss = 2.28 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735024929047
2016-08-01 17:48:25.122617: step 960, loss = 1.88 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733220815659
2016-08-01 17:48:45.013066: step 980, loss = 2.09 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.736874103546
2016-08-01 17:49:04.704701: step 1000, loss = 1.71 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.731796979904
2016-08-01 17:49:30.081625: step 1020, loss = 4.35 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.72757601738
2016-08-01 17:49:49.884360: step 1040, loss = 1.94 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.733148097992
2016-08-01 17:50:09.610781: step 1060, loss = 1.52 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731513977051
2016-08-01 17:50:29.338212: step 1080, loss = 1.56 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733726978302
2016-08-01 17:50:49.110026: step 1100, loss = 1.58 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730978012085
2016-08-01 17:51:08.949774: step 1120, loss = 1.30 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.735799074173
2016-08-01 17:51:28.713854: step 1140, loss = 1.74 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.734472036362
2016-08-01 17:51:48.391054: step 1160, loss = 2.78 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732261896133
2016-08-01 17:52:08.209567: step 1180, loss = 1.55 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729718923569
2016-08-01 17:52:27.934431: step 1200, loss = 1.41 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732175111771
2016-08-01 17:52:47.653921: step 1220, loss = 1.43 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733294010162
2016-08-01 17:53:07.374768: step 1240, loss = 1.49 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735512971878
2016-08-01 17:53:27.135359: step 1260, loss = 1.97 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.731904029846
2016-08-01 17:53:46.997229: step 1280, loss = 1.53 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732149839401
2016-08-01 17:54:06.684150: step 1300, loss = 1.58 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.730285167694
2016-08-01 17:54:26.465022: step 1320, loss = 1.38 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730490922928
2016-08-01 17:54:46.257606: step 1340, loss = 2.63 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732784032822
2016-08-01 17:55:05.973230: step 1360, loss = 5.92 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735851049423
2016-08-01 17:55:25.731986: step 1380, loss = 2.26 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.731850862503
2016-08-01 17:55:45.458025: step 1400, loss = 1.40 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729556798935
2016-08-01 17:56:05.204359: step 1420, loss = 1.54 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73067688942
2016-08-01 17:56:24.896224: step 1440, loss = 2.54 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728433847427
2016-08-01 17:56:44.555962: step 1460, loss = 1.44 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.730480909348
2016-08-01 17:57:04.260231: step 1480, loss = 2.98 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731180906296
2016-08-01 17:57:24.082821: step 1500, loss = 2.10 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731200933456
2016-08-01 17:57:43.863724: step 1520, loss = 2.45 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.737015008926
2016-08-01 17:58:03.654779: step 1540, loss = 2.26 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.731788158417
2016-08-01 17:58:23.363051: step 1560, loss = 1.46 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732521057129
2016-08-01 17:58:43.171036: step 1580, loss = 1.69 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733797073364
2016-08-01 17:59:02.834700: step 1600, loss = 1.49 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729777097702
2016-08-01 17:59:22.647984: step 1620, loss = 3.22 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730093002319
2016-08-01 17:59:42.359755: step 1640, loss = 2.13 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73272895813
2016-08-01 18:00:02.210405: step 1660, loss = 4.55 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735430955887
2016-08-01 18:00:21.956633: step 1680, loss = 2.49 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733422994614
2016-08-01 18:00:41.680320: step 1700, loss = 2.20 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733753919601
2016-08-01 18:01:01.377650: step 1720, loss = 2.33 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731739044189
2016-08-01 18:01:21.087981: step 1740, loss = 1.69 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735357999802
2016-08-01 18:01:40.859544: step 1760, loss = 1.43 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733510017395
2016-08-01 18:02:00.602688: step 1780, loss = 2.63 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.736882925034
2016-08-01 18:02:20.298508: step 1800, loss = 2.60 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.732196092606
2016-08-01 18:02:40.000765: step 1820, loss = 2.77 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.728433132172
2016-08-01 18:02:59.780309: step 1840, loss = 2.50 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.731147050858
2016-08-01 18:03:19.599748: step 1860, loss = 2.47 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731292009354
2016-08-01 18:03:39.446950: step 1880, loss = 1.94 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733597040176
2016-08-01 18:03:59.147760: step 1900, loss = 3.22 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732549905777
2016-08-01 18:04:19.101352: step 1920, loss = 2.91 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730268955231
2016-08-01 18:04:38.944369: step 1940, loss = 5.26 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734826087952
2016-08-01 18:04:58.641395: step 1960, loss = 5.19 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.730876922607
2016-08-01 18:05:18.339981: step 1980, loss = 3.17 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.734670877457
2016-08-01 18:05:38.090857: step 2000, loss = 4.21 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.730483055115
2016-08-01 18:06:06.450917: step 2020, loss = 2.28 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.735054016113
2016-08-01 18:06:26.136561: step 2040, loss = 1.71 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.73058795929
2016-08-01 18:06:45.821643: step 2060, loss = 1.75 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728834867477
2016-08-01 18:07:05.546316: step 2080, loss = 5.92 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.734310865402
2016-08-01 18:07:25.357723: step 2100, loss = 3.97 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730824947357
2016-08-01 18:07:44.990186: step 2120, loss = 2.21 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729846000671
2016-08-01 18:08:04.950315: step 2140, loss = 7.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731623888016
2016-08-01 18:08:24.614828: step 2160, loss = 3.10 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731194019318
2016-08-01 18:08:44.259955: step 2180, loss = 4.15 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733381986618
2016-08-01 18:09:03.927855: step 2200, loss = 2.35 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.729592084885
2016-08-01 18:09:23.618481: step 2220, loss = 3.90 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732665061951
2016-08-01 18:09:43.382304: step 2240, loss = 3.12 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730014801025
2016-08-01 18:10:03.082116: step 2260, loss = 4.00 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731534957886
2016-08-01 18:10:22.914332: step 2280, loss = 2.50 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731843948364
2016-08-01 18:10:42.568523: step 2300, loss = 3.05 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731735944748
2016-08-01 18:11:02.296255: step 2320, loss = 4.34 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733746051788
2016-08-01 18:11:21.992789: step 2340, loss = 2.82 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.735061168671
2016-08-01 18:11:41.621882: step 2360, loss = 4.45 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.731638908386
2016-08-01 18:12:01.344440: step 2380, loss = 3.47 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.730911016464
2016-08-01 18:12:21.117326: step 2400, loss = 3.79 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730613946915
2016-08-01 18:12:40.776141: step 2420, loss = 2.29 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731160879135
2016-08-01 18:13:00.491223: step 2440, loss = 4.27 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733304977417
2016-08-01 18:13:20.235082: step 2460, loss = 2.02 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733951091766
2016-08-01 18:13:39.935556: step 2480, loss = 1.82 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733186006546
2016-08-01 18:13:59.607203: step 2500, loss = 2.12 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.738023996353
2016-08-01 18:14:19.252883: step 2520, loss = 2.78 (5.4 examples/sec; 0.738 sec/batch)
num_examples_per_step,duration 4 0.730319976807
2016-08-01 18:14:39.254113: step 2540, loss = 3.51 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733528137207
2016-08-01 18:14:58.983139: step 2560, loss = 3.65 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733013868332
2016-08-01 18:15:18.659478: step 2580, loss = 5.20 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731204986572
2016-08-01 18:15:38.386782: step 2600, loss = 4.38 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733777999878
2016-08-01 18:15:58.028262: step 2620, loss = 3.63 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731463909149
2016-08-01 18:16:17.708911: step 2640, loss = 2.90 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.734064102173
2016-08-01 18:16:37.376628: step 2660, loss = 2.35 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729229927063
2016-08-01 18:16:56.996197: step 2680, loss = 3.46 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.735237121582
2016-08-01 18:17:16.770496: step 2700, loss = 5.30 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.734599113464
2016-08-01 18:17:36.445232: step 2720, loss = 3.12 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733583211899
2016-08-01 18:17:56.256772: step 2740, loss = 2.37 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729626893997
2016-08-01 18:18:15.959321: step 2760, loss = 8.74 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73450088501
2016-08-01 18:18:35.754076: step 2780, loss = 3.07 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.73305606842
2016-08-01 18:18:55.520865: step 2800, loss = 3.25 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.736124038696
2016-08-01 18:19:15.307740: step 2820, loss = 8.36 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.732017993927
2016-08-01 18:19:34.967632: step 2840, loss = 1.76 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732547998428
2016-08-01 18:19:54.680113: step 2860, loss = 2.75 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.73419713974
2016-08-01 18:20:14.454876: step 2880, loss = 2.67 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730251073837
2016-08-01 18:20:34.259423: step 2900, loss = 6.54 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732672929764
2016-08-01 18:20:56.032138: step 2920, loss = 4.06 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.729673147202
2016-08-01 18:21:16.075523: step 2940, loss = 3.41 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.727533817291
2016-08-01 18:21:35.811360: step 2960, loss = 3.06 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726150989532
2016-08-01 18:21:55.520725: step 2980, loss = 7.19 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729110956192
2016-08-01 18:22:15.244449: step 3000, loss = 2.57 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72863984108
2016-08-01 18:22:41.075332: step 3020, loss = 4.48 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727300882339
2016-08-01 18:23:04.053134: step 3040, loss = 3.74 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.731688976288
2016-08-01 18:23:23.799626: step 3060, loss = 6.28 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.728459119797
2016-08-01 18:23:43.496440: step 3080, loss = 3.02 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.735116958618
2016-08-01 18:24:03.229789: step 3100, loss = 4.56 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.735033035278
2016-08-01 18:24:23.139888: step 3120, loss = 4.78 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.734294891357
2016-08-01 18:24:42.819684: step 3140, loss = 3.86 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734403848648
2016-08-01 18:25:02.524102: step 3160, loss = 2.50 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732364177704
2016-08-01 18:25:22.181343: step 3180, loss = 7.00 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729140043259
2016-08-01 18:25:41.853507: step 3200, loss = 5.49 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732960939407
2016-08-01 18:26:01.552163: step 3220, loss = 3.26 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733631849289
2016-08-01 18:26:21.254104: step 3240, loss = 5.86 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732953071594
2016-08-01 18:26:41.007371: step 3260, loss = 7.27 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732985019684
2016-08-01 18:27:00.693907: step 3280, loss = 3.33 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.726886034012
2016-08-01 18:27:21.303776: step 3300, loss = 3.45 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730264186859
2016-08-01 18:27:40.988359: step 3320, loss = 2.43 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73166513443
2016-08-01 18:28:00.698318: step 3340, loss = 3.92 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.734569072723
2016-08-01 18:28:20.432491: step 3360, loss = 4.55 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.732680082321
2016-08-01 18:28:40.153360: step 3380, loss = 2.52 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733763933182
2016-08-01 18:28:59.874624: step 3400, loss = 1.80 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733259916306
2016-08-01 18:29:19.562557: step 3420, loss = 2.58 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.728081941605
2016-08-01 18:29:39.326566: step 3440, loss = 4.08 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.732245922089
2016-08-01 18:29:59.006677: step 3460, loss = 5.80 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729893922806
2016-08-01 18:30:18.808493: step 3480, loss = 1.69 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72775387764
2016-08-01 18:30:38.534550: step 3500, loss = 7.91 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.73064494133
2016-08-01 18:30:58.159338: step 3520, loss = 2.74 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730504989624
2016-08-01 18:31:17.840022: step 3540, loss = 2.45 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732710123062
2016-08-01 18:31:37.449462: step 3560, loss = 2.68 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731289863586
2016-08-01 18:31:57.099496: step 3580, loss = 6.22 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.734815120697
2016-08-01 18:32:16.749113: step 3600, loss = 5.88 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.732761144638
2016-08-01 18:32:36.431878: step 3620, loss = 2.77 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735594987869
2016-08-01 18:32:56.212650: step 3640, loss = 3.31 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.729948997498
2016-08-01 18:33:15.956980: step 3660, loss = 2.45 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728822946548
2016-08-01 18:33:35.613569: step 3680, loss = 2.26 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.734174966812
2016-08-01 18:33:57.259042: step 3700, loss = 3.71 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.73607301712
2016-08-01 18:34:16.959627: step 3720, loss = 3.89 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.732610225677
2016-08-01 18:34:36.809505: step 3740, loss = 2.35 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730175018311
2016-08-01 18:34:56.516095: step 3760, loss = 3.24 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731848955154
2016-08-01 18:35:16.260206: step 3780, loss = 2.99 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731755018234
2016-08-01 18:35:35.961507: step 3800, loss = 2.09 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.73478102684
2016-08-01 18:35:55.838621: step 3820, loss = 3.64 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.734691858292
2016-08-01 18:36:15.569988: step 3840, loss = 2.50 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.726897001266
2016-08-01 18:36:35.169029: step 3860, loss = 3.26 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725932121277
2016-08-01 18:36:54.835756: step 3880, loss = 4.33 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.731613874435
2016-08-01 18:37:14.518228: step 3900, loss = 2.82 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.730412006378
2016-08-01 18:37:34.175496: step 3920, loss = 1.84 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730371952057
2016-08-01 18:37:53.872098: step 3940, loss = 2.33 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734524011612
2016-08-01 18:38:13.691907: step 3960, loss = 6.37 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.730046033859
2016-08-01 18:38:33.512821: step 3980, loss = 3.38 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731000900269
2016-08-01 18:38:53.127775: step 4000, loss = 3.67 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727666139603
2016-08-01 18:39:18.820061: step 4020, loss = 3.12 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.732249975204
2016-08-01 18:39:38.431151: step 4040, loss = 6.14 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731329917908
2016-08-01 18:39:58.075055: step 4060, loss = 2.80 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73126411438
2016-08-01 18:40:17.726670: step 4080, loss = 1.99 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73249578476
2016-08-01 18:40:37.390856: step 4100, loss = 3.16 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.734243154526
2016-08-01 18:40:57.091652: step 4120, loss = 2.89 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.73104596138
2016-08-01 18:41:16.744169: step 4140, loss = 4.47 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733941078186
2016-08-01 18:41:36.379588: step 4160, loss = 2.40 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731621980667
2016-08-01 18:41:56.094869: step 4180, loss = 6.43 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.738931179047
2016-08-01 18:42:15.788226: step 4200, loss = 5.40 (5.4 examples/sec; 0.739 sec/batch)
num_examples_per_step,duration 4 0.733605861664
2016-08-01 18:42:35.479915: step 4220, loss = 4.03 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.736981868744
2016-08-01 18:42:55.203841: step 4240, loss = 4.27 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730451107025
2016-08-01 18:43:14.866958: step 4260, loss = 8.61 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733551979065
2016-08-01 18:43:34.523609: step 4280, loss = 3.31 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732048034668
2016-08-01 18:43:54.192890: step 4300, loss = 5.38 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731689929962
2016-08-01 18:44:13.875216: step 4320, loss = 4.41 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732502937317
2016-08-01 18:44:33.580029: step 4340, loss = 3.76 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732985019684
2016-08-01 18:44:53.332406: step 4360, loss = 2.99 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.737257957458
2016-08-01 18:45:13.065707: step 4380, loss = 6.49 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730218887329
2016-08-01 18:45:32.689997: step 4400, loss = 2.59 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729522228241
2016-08-01 18:45:52.370289: step 4420, loss = 8.49 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729383945465
2016-08-01 18:46:12.009636: step 4440, loss = 1.44 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72488617897
2016-08-01 18:46:31.667830: step 4460, loss = 3.06 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.72739481926
2016-08-01 18:46:51.388644: step 4480, loss = 13.99 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728724956512
2016-08-01 18:47:11.065648: step 4500, loss = 3.43 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728329181671
2016-08-01 18:47:30.747505: step 4520, loss = 3.26 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729496955872
2016-08-01 18:47:50.532352: step 4540, loss = 4.17 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728856086731
2016-08-01 18:48:10.262545: step 4560, loss = 4.44 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730948925018
2016-08-01 18:48:29.924876: step 4580, loss = 1.93 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731158971786
2016-08-01 18:48:49.654472: step 4600, loss = 3.18 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73412322998
2016-08-01 18:49:09.357401: step 4620, loss = 6.02 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.735140800476
2016-08-01 18:49:29.142519: step 4640, loss = 2.42 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.728123188019
2016-08-01 18:49:48.839191: step 4660, loss = 5.10 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729602098465
2016-08-01 18:50:08.543948: step 4680, loss = 14.89 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726459026337
2016-08-01 18:50:28.160360: step 4700, loss = 3.75 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.731960058212
2016-08-01 18:50:47.796996: step 4720, loss = 4.98 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731029987335
2016-08-01 18:51:07.546759: step 4740, loss = 3.23 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.735682010651
2016-08-01 18:51:27.383483: step 4760, loss = 2.68 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.73203086853
2016-08-01 18:51:47.037685: step 4780, loss = 2.02 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732547044754
2016-08-01 18:52:06.681228: step 4800, loss = 4.80 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.736485004425
2016-08-01 18:52:26.503945: step 4820, loss = 2.59 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.737402915955
2016-08-01 18:52:46.214950: step 4840, loss = 3.96 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.732052087784
2016-08-01 18:53:05.845322: step 4860, loss = 3.49 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.734164953232
2016-08-01 18:53:25.585735: step 4880, loss = 4.32 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731631040573
2016-08-01 18:53:45.295845: step 4900, loss = 3.27 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729275941849
2016-08-01 18:54:04.934451: step 4920, loss = 4.58 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730335950851
2016-08-01 18:54:24.584837: step 4940, loss = 6.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730079174042
2016-08-01 18:54:44.415072: step 4960, loss = 4.76 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730037927628
2016-08-01 18:55:04.119478: step 4980, loss = 2.08 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731766939163
2016-08-01 18:55:24.004899: step 5000, loss = 4.49 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731657981873
2016-08-01 18:55:48.926949: step 5020, loss = 8.59 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.73438000679
2016-08-01 18:56:08.581696: step 5040, loss = 2.38 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733391046524
2016-08-01 18:56:28.235333: step 5060, loss = 4.49 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730266094208
2016-08-01 18:56:47.880556: step 5080, loss = 10.14 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728950023651
2016-08-01 18:57:07.507278: step 5100, loss = 3.52 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730720996857
2016-08-01 18:57:27.297075: step 5120, loss = 2.33 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733908891678
2016-08-01 18:57:47.032551: step 5140, loss = 2.16 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.735919952393
2016-08-01 18:58:06.864427: step 5160, loss = 3.99 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.730920791626
2016-08-01 18:58:26.628103: step 5180, loss = 3.06 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731122970581
2016-08-01 18:58:46.405192: step 5200, loss = 3.21 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732925891876
2016-08-01 18:59:06.179465: step 5220, loss = 4.65 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.737215995789
2016-08-01 18:59:25.961024: step 5240, loss = 2.88 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.733680009842
2016-08-01 18:59:45.579008: step 5260, loss = 3.48 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734012842178
2016-08-01 19:00:05.276679: step 5280, loss = 4.15 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731956958771
2016-08-01 19:00:24.985648: step 5300, loss = 5.70 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.736261129379
2016-08-01 19:00:44.750305: step 5320, loss = 10.65 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.733371019363
2016-08-01 19:01:04.566724: step 5340, loss = 2.29 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730130910873
2016-08-01 19:01:24.312627: step 5360, loss = 3.09 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732429027557
2016-08-01 19:01:43.976808: step 5380, loss = 4.38 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.728055953979
2016-08-01 19:02:03.619413: step 5400, loss = 3.41 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.732907056808
2016-08-01 19:02:23.421215: step 5420, loss = 2.61 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731882095337
2016-08-01 19:02:43.061225: step 5440, loss = 3.68 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735774040222
2016-08-01 19:03:02.706025: step 5460, loss = 5.14 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.730179071426
2016-08-01 19:03:22.448468: step 5480, loss = 4.16 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734305143356
2016-08-01 19:03:42.128498: step 5500, loss = 5.80 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729982852936
2016-08-01 19:04:01.971143: step 5520, loss = 3.64 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.735147953033
2016-08-01 19:04:21.660328: step 5540, loss = 2.81 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.729825019836
2016-08-01 19:04:41.320069: step 5560, loss = 2.56 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730099916458
2016-08-01 19:05:01.058945: step 5580, loss = 2.75 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731303930283
2016-08-01 19:05:20.712309: step 5600, loss = 7.18 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729974985123
2016-08-01 19:05:40.455642: step 5620, loss = 2.12 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733371019363
2016-08-01 19:06:00.207005: step 5640, loss = 3.24 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730962991714
2016-08-01 19:06:19.864151: step 5660, loss = 4.97 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731288194656
2016-08-01 19:06:39.526964: step 5680, loss = 3.25 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.734040975571
2016-08-01 19:06:59.181425: step 5700, loss = 3.07 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730667114258
2016-08-01 19:07:18.843162: step 5720, loss = 6.91 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729243993759
2016-08-01 19:07:38.461464: step 5740, loss = 3.03 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.737450122833
2016-08-01 19:07:58.153174: step 5760, loss = 4.26 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730188131332
2016-08-01 19:08:17.834520: step 5780, loss = 4.04 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729158163071
2016-08-01 19:08:37.477199: step 5800, loss = 4.17 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.733040094376
2016-08-01 19:08:57.265648: step 5820, loss = 3.54 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733648061752
2016-08-01 19:09:16.887113: step 5840, loss = 2.25 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.73060798645
2016-08-01 19:09:36.490849: step 5860, loss = 2.59 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733532905579
2016-08-01 19:09:56.282945: step 5880, loss = 3.00 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731760978699
2016-08-01 19:10:16.105961: step 5900, loss = 5.59 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732572078705
2016-08-01 19:10:35.800966: step 5920, loss = 3.16 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.734400987625
2016-08-01 19:10:55.581340: step 5940, loss = 4.94 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729843139648
2016-08-01 19:11:15.212309: step 5960, loss = 1.98 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733188152313
2016-08-01 19:11:35.183170: step 5980, loss = 4.25 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.735942840576
2016-08-01 19:11:54.889093: step 6000, loss = 2.98 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.728631973267
2016-08-01 19:12:23.858081: step 6020, loss = 4.37 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732496976852
2016-08-01 19:12:43.712230: step 6040, loss = 2.54 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732007980347
2016-08-01 19:13:03.367593: step 6060, loss = 2.18 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731222867966
2016-08-01 19:13:23.172740: step 6080, loss = 7.06 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730378866196
2016-08-01 19:13:42.829879: step 6100, loss = 3.53 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73269200325
2016-08-01 19:14:02.595318: step 6120, loss = 3.13 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731217861176
2016-08-01 19:14:22.578837: step 6140, loss = 3.50 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732032060623
2016-08-01 19:14:42.387791: step 6160, loss = 3.61 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733333110809
2016-08-01 19:15:02.090584: step 6180, loss = 2.61 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733981132507
2016-08-01 19:15:21.730258: step 6200, loss = 2.67 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.73498415947
2016-08-01 19:15:41.399817: step 6220, loss = 3.71 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733354091644
2016-08-01 19:16:01.255133: step 6240, loss = 1.96 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733305931091
2016-08-01 19:16:21.007858: step 6260, loss = 4.24 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.729793071747
2016-08-01 19:16:40.750244: step 6280, loss = 4.47 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729020118713
2016-08-01 19:17:00.498216: step 6300, loss = 4.93 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.731110811234
2016-08-01 19:17:20.183408: step 6320, loss = 5.81 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731194972992
2016-08-01 19:17:39.916985: step 6340, loss = 2.39 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730599880219
2016-08-01 19:17:59.684300: step 6360, loss = 3.87 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732408046722
2016-08-01 19:18:19.402315: step 6380, loss = 5.29 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733613014221
2016-08-01 19:18:39.135118: step 6400, loss = 3.45 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734823942184
2016-08-01 19:18:59.847519: step 6420, loss = 4.23 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.730355024338
2016-08-01 19:19:19.607910: step 6440, loss = 8.66 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.734400987625
2016-08-01 19:19:39.248782: step 6460, loss = 4.81 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733001947403
2016-08-01 19:19:58.945506: step 6480, loss = 4.95 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731092214584
2016-08-01 19:20:18.638362: step 6500, loss = 1.95 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728713035583
2016-08-01 19:20:38.384027: step 6520, loss = 4.34 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.731441020966
2016-08-01 19:20:58.027653: step 6540, loss = 2.17 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731474876404
2016-08-01 19:21:17.860319: step 6560, loss = 2.84 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730258226395
2016-08-01 19:21:37.516492: step 6580, loss = 5.39 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731795072556
2016-08-01 19:21:57.174539: step 6600, loss = 5.58 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732218980789
2016-08-01 19:22:16.955220: step 6620, loss = 5.99 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735576868057
2016-08-01 19:22:36.606559: step 6640, loss = 3.08 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.72904086113
2016-08-01 19:22:56.313685: step 6660, loss = 5.55 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730016231537
2016-08-01 19:23:16.048446: step 6680, loss = 2.50 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732503890991
2016-08-01 19:23:35.776488: step 6700, loss = 2.33 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.737830877304
2016-08-01 19:23:55.463668: step 6720, loss = 4.09 (5.4 examples/sec; 0.738 sec/batch)
num_examples_per_step,duration 4 0.733947992325
2016-08-01 19:24:15.205225: step 6740, loss = 2.69 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730004072189
2016-08-01 19:24:34.843649: step 6760, loss = 3.08 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.737607002258
2016-08-01 19:24:54.525946: step 6780, loss = 4.43 (5.4 examples/sec; 0.738 sec/batch)
num_examples_per_step,duration 4 0.734494924545
2016-08-01 19:25:14.191133: step 6800, loss = 6.35 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733263015747
2016-08-01 19:25:33.827177: step 6820, loss = 5.53 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.728661060333
2016-08-01 19:25:53.498929: step 6840, loss = 2.66 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.73227596283
2016-08-01 19:26:13.136194: step 6860, loss = 7.85 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732834100723
2016-08-01 19:26:32.761717: step 6880, loss = 3.06 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733711004257
2016-08-01 19:26:52.523257: step 6900, loss = 2.74 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732878923416
2016-08-01 19:27:12.337726: step 6920, loss = 3.89 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731600046158
2016-08-01 19:27:32.003527: step 6940, loss = 2.54 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735805988312
2016-08-01 19:27:51.725699: step 6960, loss = 4.49 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.732406139374
2016-08-01 19:28:11.597083: step 6980, loss = 3.64 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735853910446
2016-08-01 19:28:31.209521: step 7000, loss = 2.82 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.727260828018
2016-08-01 19:28:56.500429: step 7020, loss = 2.41 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.731716156006
2016-08-01 19:29:16.144473: step 7040, loss = 2.42 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.73229598999
2016-08-01 19:29:35.800641: step 7060, loss = 2.35 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.730835914612
2016-08-01 19:29:55.456064: step 7080, loss = 4.05 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730535984039
2016-08-01 19:30:15.129691: step 7100, loss = 3.90 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731975793839
2016-08-01 19:30:34.773991: step 7120, loss = 3.47 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.736242055893
2016-08-01 19:30:54.486853: step 7140, loss = 6.40 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.731924057007
2016-08-01 19:31:14.217723: step 7160, loss = 6.27 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731004953384
2016-08-01 19:31:33.830097: step 7180, loss = 2.57 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.734391927719
2016-08-01 19:31:53.591041: step 7200, loss = 3.11 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734862089157
2016-08-01 19:32:13.227002: step 7220, loss = 2.96 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.731667995453
2016-08-01 19:32:32.940117: step 7240, loss = 1.96 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733355045319
2016-08-01 19:32:52.575725: step 7260, loss = 5.15 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731399059296
2016-08-01 19:33:12.246653: step 7280, loss = 3.32 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73112821579
2016-08-01 19:33:31.942295: step 7300, loss = 3.72 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73251414299
2016-08-01 19:33:51.631071: step 7320, loss = 3.24 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730598211288
2016-08-01 19:34:11.265578: step 7340, loss = 3.50 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731953859329
2016-08-01 19:34:30.949062: step 7360, loss = 5.22 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.734259843826
2016-08-01 19:34:50.652970: step 7380, loss = 2.75 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733812093735
2016-08-01 19:35:10.435733: step 7400, loss = 3.57 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.730427026749
2016-08-01 19:35:30.188712: step 7420, loss = 3.05 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733412981033
2016-08-01 19:35:49.936424: step 7440, loss = 5.45 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732738018036
2016-08-01 19:36:09.678081: step 7460, loss = 3.81 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.729746818542
2016-08-01 19:36:29.425849: step 7480, loss = 3.22 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.739070177078
2016-08-01 19:36:49.099342: step 7500, loss = 3.87 (5.4 examples/sec; 0.739 sec/batch)
num_examples_per_step,duration 4 0.731405973434
2016-08-01 19:37:08.842689: step 7520, loss = 2.57 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73082780838
2016-08-01 19:37:28.557970: step 7540, loss = 8.52 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.73931479454
2016-08-01 19:37:48.228984: step 7560, loss = 5.18 (5.4 examples/sec; 0.739 sec/batch)
num_examples_per_step,duration 4 0.734404087067
2016-08-01 19:38:07.878349: step 7580, loss = 4.04 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729910135269
2016-08-01 19:38:27.657922: step 7600, loss = 3.48 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73453783989
2016-08-01 19:38:47.331651: step 7620, loss = 3.00 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.732764005661
2016-08-01 19:39:06.961189: step 7640, loss = 2.88 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.728640794754
2016-08-01 19:39:26.684882: step 7660, loss = 2.37 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.73948597908
2016-08-01 19:39:46.358405: step 7680, loss = 4.26 (5.4 examples/sec; 0.739 sec/batch)
num_examples_per_step,duration 4 0.72839307785
2016-08-01 19:40:05.988761: step 7700, loss = 2.10 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729681968689
2016-08-01 19:40:25.615804: step 7720, loss = 3.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.739202022552
2016-08-01 19:40:45.275900: step 7740, loss = 3.82 (5.4 examples/sec; 0.739 sec/batch)
num_examples_per_step,duration 4 0.730036973953
2016-08-01 19:41:05.098834: step 7760, loss = 4.49 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731636047363
2016-08-01 19:41:24.794514: step 7780, loss = 2.95 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735017061234
2016-08-01 19:41:44.477360: step 7800, loss = 2.69 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.727380990982
2016-08-01 19:42:04.168115: step 7820, loss = 5.46 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72758603096
2016-08-01 19:42:23.922701: step 7840, loss = 5.41 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.731667995453
2016-08-01 19:42:43.667281: step 7860, loss = 3.04 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732789993286
2016-08-01 19:43:03.281080: step 7880, loss = 4.06 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730225086212
2016-08-01 19:43:22.910449: step 7900, loss = 4.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730603933334
2016-08-01 19:43:42.651277: step 7920, loss = 3.14 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.732519865036
2016-08-01 19:44:02.302116: step 7940, loss = 3.52 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.734507083893
2016-08-01 19:44:22.055699: step 7960, loss = 5.02 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.731076002121
2016-08-01 19:44:44.544472: step 7980, loss = 3.22 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730476140976
2016-08-01 19:45:04.203472: step 8000, loss = 5.34 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726459980011
2016-08-01 19:45:28.896756: step 8020, loss = 3.94 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727796077728
2016-08-01 19:45:48.535502: step 8040, loss = 4.33 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.732517957687
2016-08-01 19:46:08.165519: step 8060, loss = 3.02 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731364965439
2016-08-01 19:46:27.910452: step 8080, loss = 6.17 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727090120316
2016-08-01 19:46:47.591359: step 8100, loss = 3.72 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729405164719
2016-08-01 19:47:07.338711: step 8120, loss = 2.89 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732750892639
2016-08-01 19:47:27.040200: step 8140, loss = 3.06 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732383966446
2016-08-01 19:47:46.752502: step 8160, loss = 2.48 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729593992233
2016-08-01 19:48:06.549562: step 8180, loss = 3.56 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73317193985
2016-08-01 19:48:26.193463: step 8200, loss = 2.91 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732279777527
2016-08-01 19:48:45.892009: step 8220, loss = 1.69 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733091831207
2016-08-01 19:49:05.706523: step 8240, loss = 3.11 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.728749036789
2016-08-01 19:49:25.462895: step 8260, loss = 2.63 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730353116989
2016-08-01 19:49:45.143627: step 8280, loss = 6.85 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731835126877
2016-08-01 19:50:04.883748: step 8300, loss = 4.07 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.732756853104
2016-08-01 19:50:24.508969: step 8320, loss = 2.70 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.731189012527
2016-08-01 19:50:44.152787: step 8340, loss = 5.88 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731607913971
2016-08-01 19:51:03.754506: step 8360, loss = 2.09 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.735066890717
2016-08-01 19:51:23.460613: step 8380, loss = 2.78 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.728968143463
2016-08-01 19:51:43.121870: step 8400, loss = 5.76 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.735738992691
2016-08-01 19:52:02.770386: step 8420, loss = 2.66 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.732516050339
2016-08-01 19:52:22.495406: step 8440, loss = 4.76 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.733637094498
2016-08-01 19:52:42.207983: step 8460, loss = 4.00 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733737230301
2016-08-01 19:53:01.871301: step 8480, loss = 3.26 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.736561059952
2016-08-01 19:53:21.859098: step 8500, loss = 6.09 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.730628013611
2016-08-01 19:53:41.582820: step 8520, loss = 3.58 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733756065369
2016-08-01 19:54:01.242274: step 8540, loss = 2.95 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.728631019592
2016-08-01 19:54:20.911263: step 8560, loss = 3.77 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729593038559
2016-08-01 19:54:40.595239: step 8580, loss = 2.93 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729485988617
2016-08-01 19:55:00.251250: step 8600, loss = 4.94 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732157945633
2016-08-01 19:55:19.881334: step 8620, loss = 3.60 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.733424186707
2016-08-01 19:55:39.594561: step 8640, loss = 3.21 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.740343809128
2016-08-01 19:55:59.226208: step 8660, loss = 4.27 (5.4 examples/sec; 0.740 sec/batch)
num_examples_per_step,duration 4 0.733982086182
2016-08-01 19:56:18.874338: step 8680, loss = 2.58 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.73094201088
2016-08-01 19:56:38.532826: step 8700, loss = 2.22 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.731894016266
2016-08-01 19:56:58.170214: step 8720, loss = 1.93 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731048107147
2016-08-01 19:57:17.793589: step 8740, loss = 2.73 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.733413934708
2016-08-01 19:57:37.448127: step 8760, loss = 3.80 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730865955353
2016-08-01 19:57:57.068415: step 8780, loss = 3.15 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730139017105
2016-08-01 19:58:16.724794: step 8800, loss = 2.40 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.735202074051
2016-08-01 19:58:36.395924: step 8820, loss = 3.96 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733850955963
2016-08-01 19:58:56.045320: step 8840, loss = 4.57 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734843015671
2016-08-01 19:59:15.850562: step 8860, loss = 3.40 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.737367153168
2016-08-01 19:59:35.562889: step 8880, loss = 2.23 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.729090929031
2016-08-01 19:59:55.329441: step 8900, loss = 2.27 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730949163437
2016-08-01 20:00:15.055931: step 8920, loss = 4.35 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728600978851
2016-08-01 20:00:34.791737: step 8940, loss = 5.74 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728704929352
2016-08-01 20:00:54.516884: step 8960, loss = 1.90 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730231046677
2016-08-01 20:01:14.233158: step 8980, loss = 3.49 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730243206024
2016-08-01 20:01:33.912305: step 9000, loss = 4.38 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726440906525
2016-08-01 20:02:00.725810: step 9020, loss = 5.84 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727323055267
2016-08-01 20:02:20.338583: step 9040, loss = 2.22 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.734084129333
2016-08-01 20:02:40.055341: step 9060, loss = 1.62 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.732131004333
2016-08-01 20:02:59.710219: step 9080, loss = 4.81 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.728952884674
2016-08-01 20:03:19.403655: step 9100, loss = 4.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.733081102371
2016-08-01 20:03:39.230606: step 9120, loss = 2.63 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732097148895
2016-08-01 20:03:58.875985: step 9140, loss = 1.59 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731600046158
2016-08-01 20:04:18.647340: step 9160, loss = 3.87 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.730406999588
2016-08-01 20:04:38.309522: step 9180, loss = 3.32 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72856593132
2016-08-01 20:04:57.945808: step 9200, loss = 4.04 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732749938965
2016-08-01 20:05:17.625515: step 9220, loss = 4.01 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.728382110596
2016-08-01 20:05:37.333133: step 9240, loss = 2.52 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728929042816
2016-08-01 20:05:57.058671: step 9260, loss = 2.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.734502077103
2016-08-01 20:06:16.823616: step 9280, loss = 10.65 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.730497837067
2016-08-01 20:06:36.464861: step 9300, loss = 4.16 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731205940247
2016-08-01 20:06:56.097200: step 9320, loss = 4.33 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.735936164856
2016-08-01 20:07:15.844099: step 9340, loss = 4.87 (5.4 examples/sec; 0.736 sec/batch)
num_examples_per_step,duration 4 0.729363918304
2016-08-01 20:07:35.752591: step 9360, loss = 3.57 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.735082864761
2016-08-01 20:07:55.727362: step 9380, loss = 4.21 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733914136887
2016-08-01 20:08:15.415015: step 9400, loss = 2.92 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.738467931747
2016-08-01 20:08:35.092877: step 9420, loss = 3.67 (5.4 examples/sec; 0.738 sec/batch)
num_examples_per_step,duration 4 0.729444026947
2016-08-01 20:08:54.770859: step 9440, loss = 3.68 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.734962940216
2016-08-01 20:09:14.609306: step 9460, loss = 2.07 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733652830124
2016-08-01 20:09:34.316525: step 9480, loss = 2.51 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.734723091125
2016-08-01 20:09:53.991355: step 9500, loss = 4.53 (5.4 examples/sec; 0.735 sec/batch)
num_examples_per_step,duration 4 0.733580112457
2016-08-01 20:10:13.696077: step 9520, loss = 3.88 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733675003052
2016-08-01 20:10:33.427643: step 9540, loss = 3.87 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.729207992554
2016-08-01 20:10:53.210778: step 9560, loss = 6.30 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730189800262
2016-08-01 20:11:12.915029: step 9580, loss = 2.45 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731971979141
2016-08-01 20:11:32.707505: step 9600, loss = 2.58 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731750011444
2016-08-01 20:11:52.373431: step 9620, loss = 3.46 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.731330871582
2016-08-01 20:12:12.016297: step 9640, loss = 2.79 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729882955551
2016-08-01 20:12:31.687510: step 9660, loss = 4.86 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73352098465
2016-08-01 20:12:51.399486: step 9680, loss = 2.37 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.733309984207
2016-08-01 20:13:11.046475: step 9700, loss = 4.90 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.732002019882
2016-08-01 20:13:30.676501: step 9720, loss = 2.95 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729669094086
2016-08-01 20:13:50.501101: step 9740, loss = 2.02 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733994007111
2016-08-01 20:14:10.308069: step 9760, loss = 2.94 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731466054916
2016-08-01 20:14:29.986633: step 9780, loss = 3.55 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728430986404
2016-08-01 20:14:49.658793: step 9800, loss = 3.94 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.731140851974
2016-08-01 20:15:09.418260: step 9820, loss = 4.80 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729583978653
2016-08-01 20:15:29.075211: step 9840, loss = 3.25 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733907938004
2016-08-01 20:15:48.728250: step 9860, loss = 6.48 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.731465101242
2016-08-01 20:16:08.277478: step 9880, loss = 2.39 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727494001389
2016-08-01 20:16:27.804245: step 9900, loss = 3.54 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729259967804
2016-08-01 20:16:47.278853: step 9920, loss = 2.33 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727187156677
2016-08-01 20:17:06.751963: step 9940, loss = 5.73 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725424051285
2016-08-01 20:17:26.179838: step 9960, loss = 5.31 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727800130844
2016-08-01 20:17:45.628989: step 9980, loss = 4.25 (5.5 examples/sec; 0.728 sec/batch)
+ date
Mon Aug  1 20:18:10 EDT 2016
