+ date
Mon Aug  1 17:32:57 EDT 2016
+ export MODULEPATH=/home/welling/git/bridges_modules::/opt/modulefiles
+ MODULEPATH=/home/welling/git/bridges_modules::/opt/modulefiles
+ module load cuda/7.5
++ /usr/bin/modulecmd bash load cuda/7.5
+ eval CUDA_ROOT=/opt/packages/cuda/7.5 ';export' 'CUDA_ROOT;LD_LIBRARY_PATH=/opt/packages/cuda/7.5/lib64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4:/opt/intel/debugger_2016/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64_lin/gcc4.4:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64_lin' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5' ';export' 'LOADEDMODULES;PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin' ';export' 'PATH;_LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5' ';export' '_LMFILES_;'
++ CUDA_ROOT=/opt/packages/cuda/7.5
++ export CUDA_ROOT
++ LD_LIBRARY_PATH=/opt/packages/cuda/7.5/lib64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4:/opt/intel/debugger_2016/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2016.3.210/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64_lin/gcc4.4:/opt/intel/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64_lin
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5
++ export LOADEDMODULES
++ PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ export PATH
++ _LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5
++ export _LMFILES_
+ module load tensorflow/0.9.0
++ /usr/bin/modulecmd bash load tensorflow/0.9.0
+ eval LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5:tensorflow/0.9.0 ';export' 'LOADEDMODULES;TENSORFLOW_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv' ';export' 'TENSORFLOW_ENV;_LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5:/home/welling/git/bridges_modules/tensorflow/0.9.0' ';export' '_LMFILES_;'
++ LOADEDMODULES=psc_path/1.0:slurm/15.08.8:intel/compilers:mpi/intel_mpi:icc/16.0.3:cuda/7.5:tensorflow/0.9.0
++ export LOADEDMODULES
++ TENSORFLOW_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ export TENSORFLOW_ENV
++ _LMFILES_=/opt/modulefiles/psc_path/1.0:/opt/modulefiles/slurm/15.08.8:/opt/modulefiles/intel/compilers:/opt/modulefiles/mpi/intel_mpi:/opt/modulefiles/icc/16.0.3:/home/welling/git/bridges_modules/cuda/7.5:/home/welling/git/bridges_modules/tensorflow/0.9.0
++ export _LMFILES_
+ source /opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv/bin/activate
++ deactivate nondestructive
++ unset -f pydoc
++ '[' -z '' ']'
++ '[' -z '' ']'
++ '[' -n /bin/bash ']'
++ hash -r
++ '[' -z '' ']'
++ unset VIRTUAL_ENV
++ '[' '!' nondestructive = nondestructive ']'
++ VIRTUAL_ENV=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ export VIRTUAL_ENV
++ _OLD_VIRTUAL_PATH=/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ PATH=/opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv/bin:/opt/packages/cuda/7.5/bin:/usr/lib64/qt-3.3/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2016.3.210/linux/bin/intel64:/opt/intel/debugger_2016/gdb/intel64_mic/bin:/opt/packages/slurm/15.08.8/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/puppet/bin:/opt/packages/slash2/psc/sbin:/opt/packages/allocations:/opt/packages/interact/bin:/opt/puppetlabs/bin:/home/liyunshq/.local/bin:/home/liyunshq/bin
++ export PATH
++ '[' -z '' ']'
++ '[' -z '' ']'
++ _OLD_VIRTUAL_PS1=
++ '[' x '!=' x ']'
+++ basename /opt/packages/TensorFlow/TensorFlow_0.9.0/TensorFlowEnv
++ PS1='(TensorFlowEnv) '
++ export PS1
++ alias pydoc
++ '[' -n /bin/bash ']'
++ hash -r
+ python segnet_train.py
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:83:00.0)
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 8.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3321 get requests, put_count=3309 evicted_count=1000 eviction_rate=0.302206 and unsatisfied allocation rate=0.334839
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
cnn inputs
batch of images (4, 360, 480, 3)
batch of labels (4, 360, 480, 1)
covn1 (4, 360, 480, 64)
pool1 (4, 180, 240, 64)
covn2 (4, 180, 240, 64)
pool2 (4, 90, 120, 64)
covn3 (4, 90, 120, 64)
pool3 (4, 45, 60, 64)
covn4 (4, 45, 60, 64)
pool4 (4, 23, 30, 64)
up4 (4, 45, 60, 64)
de_covn4 (4, 45, 60, 64)
up3 (4, 90, 120, 64)
de_covn3 (4, 90, 120, 64)
up2 (4, 180, 240, 64)
de_covn2 (4, 180, 240, 64)
up1 (4, 360, 480, 64)
de_covn1 (4, 360, 480, 64)
start a session
start on training
num_examples_per_step,duration 4 4.36324810982
2016-08-01 17:33:08.555087: step 0, loss = 2.25 (0.9 examples/sec; 4.363 sec/batch)
num_examples_per_step,duration 4 0.708971977234
2016-08-01 17:33:36.918886: step 20, loss = 16.15 (5.6 examples/sec; 0.709 sec/batch)
num_examples_per_step,duration 4 0.711625099182
2016-08-01 17:33:56.047109: step 40, loss = 16.88 (5.6 examples/sec; 0.712 sec/batch)
num_examples_per_step,duration 4 0.70920085907
2016-08-01 17:34:15.182854: step 60, loss = 16.37 (5.6 examples/sec; 0.709 sec/batch)
num_examples_per_step,duration 4 0.71475815773
2016-08-01 17:34:34.504306: step 80, loss = 20.53 (5.6 examples/sec; 0.715 sec/batch)
num_examples_per_step,duration 4 0.715128898621
2016-08-01 17:34:53.716913: step 100, loss = 20.81 (5.6 examples/sec; 0.715 sec/batch)
num_examples_per_step,duration 4 0.713146924973
2016-08-01 17:35:12.883567: step 120, loss = 22.22 (5.6 examples/sec; 0.713 sec/batch)
num_examples_per_step,duration 4 0.716682910919
2016-08-01 17:35:32.180693: step 140, loss = 17.61 (5.6 examples/sec; 0.717 sec/batch)
num_examples_per_step,duration 4 0.717221021652
2016-08-01 17:35:51.681489: step 160, loss = 19.36 (5.6 examples/sec; 0.717 sec/batch)
num_examples_per_step,duration 4 0.715564012527
2016-08-01 17:36:11.204510: step 180, loss = 19.41 (5.6 examples/sec; 0.716 sec/batch)
num_examples_per_step,duration 4 0.720533132553
2016-08-01 17:36:30.612585: step 200, loss = 11.60 (5.6 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.724607944489
2016-08-01 17:36:50.134188: step 220, loss = 14.61 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.722795009613
2016-08-01 17:37:09.720943: step 240, loss = 17.41 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.724831104279
2016-08-01 17:37:29.202914: step 260, loss = 10.71 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724542856216
2016-08-01 17:37:48.728063: step 280, loss = 9.86 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726531028748
2016-08-01 17:38:11.530857: step 300, loss = 9.22 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727200984955
2016-08-01 17:38:31.140333: step 320, loss = 9.19 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727675914764
2016-08-01 17:38:50.744278: step 340, loss = 26.07 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72505903244
2016-08-01 17:39:10.397690: step 360, loss = 9.55 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726493120193
2016-08-01 17:39:29.971517: step 380, loss = 8.66 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727313041687
2016-08-01 17:39:49.544113: step 400, loss = 12.06 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727448940277
2016-08-01 17:40:10.529293: step 420, loss = 8.75 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.723885059357
2016-08-01 17:40:30.190711: step 440, loss = 16.90 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.731997013092
2016-08-01 17:40:49.924547: step 460, loss = 11.30 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.728878974915
2016-08-01 17:41:09.473076: step 480, loss = 11.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725255966187
2016-08-01 17:41:29.041658: step 500, loss = 9.36 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728238105774
2016-08-01 17:41:48.645028: step 520, loss = 7.79 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727430105209
2016-08-01 17:42:08.085720: step 540, loss = 14.99 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726149082184
2016-08-01 17:42:27.853191: step 560, loss = 13.03 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724472045898
2016-08-01 17:42:47.450059: step 580, loss = 7.78 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.723833799362
2016-08-01 17:43:07.054701: step 600, loss = 9.54 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.729596138
2016-08-01 17:43:26.658320: step 620, loss = 8.61 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726164102554
2016-08-01 17:43:46.435443: step 640, loss = 9.02 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72781586647
2016-08-01 17:44:06.002793: step 660, loss = 7.05 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727072000504
2016-08-01 17:44:25.665106: step 680, loss = 10.06 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725775003433
2016-08-01 17:44:45.373660: step 700, loss = 9.35 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727926969528
2016-08-01 17:45:04.954553: step 720, loss = 7.44 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72892999649
2016-08-01 17:45:24.587002: step 740, loss = 9.19 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727513074875
2016-08-01 17:45:44.171539: step 760, loss = 7.21 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72634601593
2016-08-01 17:46:03.760548: step 780, loss = 6.50 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727047920227
2016-08-01 17:46:23.403324: step 800, loss = 6.25 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725892066956
2016-08-01 17:46:46.091563: step 820, loss = 10.86 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724064826965
2016-08-01 17:47:05.658884: step 840, loss = 6.11 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.727467060089
2016-08-01 17:47:25.267388: step 860, loss = 7.65 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727166891098
2016-08-01 17:47:45.031188: step 880, loss = 7.32 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726869106293
2016-08-01 17:48:04.818552: step 900, loss = 10.47 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728147029877
2016-08-01 17:48:24.512285: step 920, loss = 8.08 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72883105278
2016-08-01 17:48:44.278471: step 940, loss = 8.07 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727416038513
2016-08-01 17:49:04.044259: step 960, loss = 11.80 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725740909576
2016-08-01 17:49:23.694996: step 980, loss = 9.85 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.730104923248
2016-08-01 17:49:43.326502: step 1000, loss = 11.17 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729963779449
2016-08-01 17:50:09.441003: step 1020, loss = 7.14 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729021072388
2016-08-01 17:50:29.180574: step 1040, loss = 11.09 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730571985245
2016-08-01 17:50:48.883302: step 1060, loss = 10.91 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729454994202
2016-08-01 17:51:08.477434: step 1080, loss = 9.66 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728965044022
2016-08-01 17:51:28.045115: step 1100, loss = 12.14 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730278968811
2016-08-01 17:51:47.712661: step 1120, loss = 11.37 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728613853455
2016-08-01 17:52:07.380553: step 1140, loss = 10.32 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.722624778748
2016-08-01 17:52:26.937001: step 1160, loss = 10.26 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.725008964539
2016-08-01 17:52:46.490241: step 1180, loss = 12.86 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725584983826
2016-08-01 17:53:05.971562: step 1200, loss = 15.36 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726022958755
2016-08-01 17:53:25.534046: step 1220, loss = 14.93 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727725028992
2016-08-01 17:53:45.139507: step 1240, loss = 13.41 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.724250078201
2016-08-01 17:54:04.853817: step 1260, loss = 14.16 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.729192972183
2016-08-01 17:54:24.395422: step 1280, loss = 17.59 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72798705101
2016-08-01 17:54:44.039760: step 1300, loss = 18.14 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727061986923
2016-08-01 17:55:03.600643: step 1320, loss = 14.40 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.720484018326
2016-08-01 17:55:23.167859: step 1340, loss = 15.06 (5.6 examples/sec; 0.720 sec/batch)
num_examples_per_step,duration 4 0.723967075348
2016-08-01 17:55:42.689211: step 1360, loss = 19.10 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.724254131317
2016-08-01 17:56:02.198785: step 1380, loss = 33.58 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.723546028137
2016-08-01 17:56:21.771477: step 1400, loss = 17.88 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.72974395752
2016-08-01 17:56:41.432567: step 1420, loss = 21.53 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72665309906
2016-08-01 17:57:00.924594: step 1440, loss = 23.05 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726542949677
2016-08-01 17:57:20.644608: step 1460, loss = 19.72 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72746014595
2016-08-01 17:57:40.216933: step 1480, loss = 23.00 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728937864304
2016-08-01 17:57:59.852131: step 1500, loss = 21.20 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728856086731
2016-08-01 17:58:19.386685: step 1520, loss = 23.19 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.731257915497
2016-08-01 17:58:38.837856: step 1540, loss = 26.03 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727880954742
2016-08-01 17:58:58.397853: step 1560, loss = 23.01 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725253105164
2016-08-01 17:59:17.905919: step 1580, loss = 30.83 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725404024124
2016-08-01 17:59:37.514290: step 1600, loss = 26.28 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727389097214
2016-08-01 17:59:57.042303: step 1620, loss = 23.84 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726925849915
2016-08-01 18:00:16.726117: step 1640, loss = 25.34 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727994918823
2016-08-01 18:00:36.370606: step 1660, loss = 33.52 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725788831711
2016-08-01 18:00:55.976054: step 1680, loss = 29.60 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726874113083
2016-08-01 18:01:15.663973: step 1700, loss = 29.06 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726552963257
2016-08-01 18:01:35.281519: step 1720, loss = 28.29 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729759931564
2016-08-01 18:01:55.097008: step 1740, loss = 34.34 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726551055908
2016-08-01 18:02:14.679511: step 1760, loss = 32.15 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727475166321
2016-08-01 18:02:34.189588: step 1780, loss = 33.52 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730814933777
2016-08-01 18:02:53.758162: step 1800, loss = 30.56 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727928161621
2016-08-01 18:03:13.464944: step 1820, loss = 38.51 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.724735975266
2016-08-01 18:03:32.980545: step 1840, loss = 39.12 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727097034454
2016-08-01 18:03:52.553321: step 1860, loss = 37.24 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728582859039
2016-08-01 18:04:12.136560: step 1880, loss = 37.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.722409963608
2016-08-01 18:04:31.663125: step 1900, loss = 35.19 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.72501206398
2016-08-01 18:04:51.344568: step 1920, loss = 38.00 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724608898163
2016-08-01 18:05:10.848558: step 1940, loss = 37.69 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.723458051682
2016-08-01 18:05:30.437612: step 1960, loss = 42.26 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.724082946777
2016-08-01 18:05:49.954169: step 1980, loss = 37.75 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725182056427
2016-08-01 18:06:09.484081: step 2000, loss = 58.91 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729643106461
2016-08-01 18:06:35.448471: step 2020, loss = 39.70 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726199150085
2016-08-01 18:06:54.909870: step 2040, loss = 48.25 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728599071503
2016-08-01 18:07:14.412405: step 2060, loss = 49.30 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726598024368
2016-08-01 18:07:33.955780: step 2080, loss = 45.74 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72696018219
2016-08-01 18:07:53.502421: step 2100, loss = 48.63 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728076934814
2016-08-01 18:08:13.082714: step 2120, loss = 44.79 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.731656074524
2016-08-01 18:08:32.653715: step 2140, loss = 45.93 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.725764036179
2016-08-01 18:08:52.251042: step 2160, loss = 49.37 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724575042725
2016-08-01 18:09:11.741553: step 2180, loss = 53.97 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726351976395
2016-08-01 18:09:31.240767: step 2200, loss = 50.04 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729948997498
2016-08-01 18:09:50.984662: step 2220, loss = 53.09 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.732610940933
2016-08-01 18:10:10.673796: step 2240, loss = 51.19 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.727885007858
2016-08-01 18:10:30.335656: step 2260, loss = 50.77 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726453781128
2016-08-01 18:10:49.861646: step 2280, loss = 59.68 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729324102402
2016-08-01 18:11:09.371009: step 2300, loss = 54.96 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729387044907
2016-08-01 18:11:29.018441: step 2320, loss = 57.08 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729949951172
2016-08-01 18:11:48.564599: step 2340, loss = 64.10 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726574897766
2016-08-01 18:12:08.052688: step 2360, loss = 55.70 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729704856873
2016-08-01 18:12:27.713995: step 2380, loss = 57.09 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728348970413
2016-08-01 18:12:47.258913: step 2400, loss = 57.28 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729598045349
2016-08-01 18:13:06.816925: step 2420, loss = 64.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.727949142456
2016-08-01 18:13:26.364747: step 2440, loss = 64.66 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729583978653
2016-08-01 18:13:46.006839: step 2460, loss = 59.09 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.725352048874
2016-08-01 18:14:05.592873: step 2480, loss = 59.81 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727236032486
2016-08-01 18:14:25.158881: step 2500, loss = 66.13 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724421024323
2016-08-01 18:14:44.686915: step 2520, loss = 65.25 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725805997849
2016-08-01 18:15:04.227699: step 2540, loss = 67.19 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724871873856
2016-08-01 18:15:23.738918: step 2560, loss = 67.85 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728717088699
2016-08-01 18:15:43.386817: step 2580, loss = 68.16 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72839307785
2016-08-01 18:16:03.013551: step 2600, loss = 78.12 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727740049362
2016-08-01 18:16:22.515476: step 2620, loss = 70.46 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725250959396
2016-08-01 18:16:42.367045: step 2640, loss = 71.75 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727718114853
2016-08-01 18:17:01.876657: step 2660, loss = 82.65 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72585606575
2016-08-01 18:17:21.424026: step 2680, loss = 82.42 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728351831436
2016-08-01 18:17:40.971803: step 2700, loss = 75.79 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729501008987
2016-08-01 18:18:00.568578: step 2720, loss = 72.60 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.724624872208
2016-08-01 18:18:20.200693: step 2740, loss = 75.29 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728150844574
2016-08-01 18:18:39.733864: step 2760, loss = 74.76 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729418992996
2016-08-01 18:18:59.422862: step 2780, loss = 79.90 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728327989578
2016-08-01 18:19:18.964615: step 2800, loss = 74.75 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.724947929382
2016-08-01 18:19:38.563904: step 2820, loss = 80.69 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729538917542
2016-08-01 18:19:58.110205: step 2840, loss = 83.17 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.724889039993
2016-08-01 18:20:17.729757: step 2860, loss = 89.42 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725260019302
2016-08-01 18:20:37.474755: step 2880, loss = 82.80 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727816104889
2016-08-01 18:20:57.071792: step 2900, loss = 84.48 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.732954978943
2016-08-01 18:21:16.666957: step 2920, loss = 96.93 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.730012178421
2016-08-01 18:21:36.164545: step 2940, loss = 86.44 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72779917717
2016-08-01 18:21:55.706689: step 2960, loss = 83.86 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728447914124
2016-08-01 18:22:15.283856: step 2980, loss = 85.48 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726937055588
2016-08-01 18:22:34.923253: step 3000, loss = 86.19 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729981899261
2016-08-01 18:23:04.730294: step 3020, loss = 87.42 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.724687099457
2016-08-01 18:23:24.327213: step 3040, loss = 95.26 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.72984790802
2016-08-01 18:23:43.934423: step 3060, loss = 91.50 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726707935333
2016-08-01 18:24:03.493184: step 3080, loss = 92.30 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725290775299
2016-08-01 18:24:23.284788: step 3100, loss = 91.84 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727443933487
2016-08-01 18:24:42.917671: step 3120, loss = 96.41 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727303028107
2016-08-01 18:25:02.519834: step 3140, loss = 93.05 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72759103775
2016-08-01 18:25:22.162846: step 3160, loss = 92.73 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.721771001816
2016-08-01 18:25:41.794172: step 3180, loss = 91.28 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.726921081543
2016-08-01 18:26:01.403686: step 3200, loss = 101.52 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725698947906
2016-08-01 18:26:20.926031: step 3220, loss = 100.61 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.731560945511
2016-08-01 18:26:40.465686: step 3240, loss = 98.19 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.729121923447
2016-08-01 18:27:00.098615: step 3260, loss = 96.30 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.73045706749
2016-08-01 18:27:19.785928: step 3280, loss = 97.65 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729048013687
2016-08-01 18:27:39.388256: step 3300, loss = 103.80 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727165937424
2016-08-01 18:27:59.014158: step 3320, loss = 105.02 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724967002869
2016-08-01 18:28:18.749745: step 3340, loss = 98.67 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727082014084
2016-08-01 18:28:38.376342: step 3360, loss = 101.49 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730980157852
2016-08-01 18:28:58.099737: step 3380, loss = 100.79 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.729187011719
2016-08-01 18:29:17.676456: step 3400, loss = 104.60 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728579044342
2016-08-01 18:29:37.401617: step 3420, loss = 110.75 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727271795273
2016-08-01 18:29:56.940068: step 3440, loss = 103.98 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.722877979279
2016-08-01 18:30:16.574773: step 3460, loss = 103.04 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.723549127579
2016-08-01 18:30:36.270994: step 3480, loss = 107.70 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.721843004227
2016-08-01 18:30:55.978422: step 3500, loss = 108.73 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.720987081528
2016-08-01 18:31:15.546789: step 3520, loss = 108.21 (5.5 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.729462862015
2016-08-01 18:31:38.629066: step 3540, loss = 109.26 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726313114166
2016-08-01 18:31:58.251279: step 3560, loss = 109.30 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.730327129364
2016-08-01 18:32:17.821261: step 3580, loss = 114.30 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726716041565
2016-08-01 18:32:37.420393: step 3600, loss = 107.66 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729849100113
2016-08-01 18:32:57.025304: step 3620, loss = 114.11 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729626893997
2016-08-01 18:33:16.588012: step 3640, loss = 116.44 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729017019272
2016-08-01 18:33:36.152536: step 3660, loss = 114.96 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726894140244
2016-08-01 18:33:55.773736: step 3680, loss = 112.87 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726250886917
2016-08-01 18:34:15.367968: step 3700, loss = 113.99 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.723541021347
2016-08-01 18:34:34.975744: step 3720, loss = 118.86 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.723613023758
2016-08-01 18:34:54.582599: step 3740, loss = 112.62 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.724191904068
2016-08-01 18:35:14.173890: step 3760, loss = 115.49 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.720248937607
2016-08-01 18:35:33.901080: step 3780, loss = 120.03 (5.6 examples/sec; 0.720 sec/batch)
num_examples_per_step,duration 4 0.721639871597
2016-08-01 18:35:53.504510: step 3800, loss = 115.92 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.725857973099
2016-08-01 18:36:13.074756: step 3820, loss = 117.34 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.725453853607
2016-08-01 18:36:32.720343: step 3840, loss = 117.57 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.72595500946
2016-08-01 18:36:52.323038: step 3860, loss = 117.80 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729418039322
2016-08-01 18:37:11.959562: step 3880, loss = 120.46 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.724901914597
2016-08-01 18:37:31.583590: step 3900, loss = 119.37 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.72713804245
2016-08-01 18:37:51.178506: step 3920, loss = 121.02 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727777957916
2016-08-01 18:38:10.785253: step 3940, loss = 121.75 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727306127548
2016-08-01 18:38:30.526888: step 3960, loss = 121.98 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730574131012
2016-08-01 18:38:50.250970: step 3980, loss = 122.28 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.72842502594
2016-08-01 18:39:09.899093: step 4000, loss = 121.90 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727980136871
2016-08-01 18:39:36.143121: step 4020, loss = 136.07 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72673201561
2016-08-01 18:39:55.739142: step 4040, loss = 125.98 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72722697258
2016-08-01 18:40:15.371554: step 4060, loss = 123.12 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729212999344
2016-08-01 18:40:35.007097: step 4080, loss = 124.45 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728303909302
2016-08-01 18:40:54.611684: step 4100, loss = 124.57 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729258060455
2016-08-01 18:41:14.259752: step 4120, loss = 129.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729230880737
2016-08-01 18:41:33.947778: step 4140, loss = 132.96 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.723737001419
2016-08-01 18:41:53.502959: step 4160, loss = 134.71 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.72484087944
2016-08-01 18:42:13.058622: step 4180, loss = 134.82 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725654840469
2016-08-01 18:42:32.676777: step 4200, loss = 130.63 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724117040634
2016-08-01 18:42:52.368380: step 4220, loss = 132.45 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.727041959763
2016-08-01 18:43:11.998312: step 4240, loss = 129.11 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729282140732
2016-08-01 18:43:31.613041: step 4260, loss = 134.27 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725224971771
2016-08-01 18:43:51.144808: step 4280, loss = 137.73 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.733568191528
2016-08-01 18:44:10.719768: step 4300, loss = 132.95 (5.5 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.728582143784
2016-08-01 18:44:30.319269: step 4320, loss = 134.50 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730505943298
2016-08-01 18:44:49.951711: step 4340, loss = 133.30 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.727784156799
2016-08-01 18:45:09.475967: step 4360, loss = 138.83 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.722924947739
2016-08-01 18:45:29.073031: step 4380, loss = 135.78 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.72171998024
2016-08-01 18:45:48.756497: step 4400, loss = 134.98 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.721366167068
2016-08-01 18:46:08.378981: step 4420, loss = 135.69 (5.5 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.727326154709
2016-08-01 18:46:27.988035: step 4440, loss = 138.82 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724449872971
2016-08-01 18:46:49.194407: step 4460, loss = 134.73 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.722927093506
2016-08-01 18:47:08.908650: step 4480, loss = 136.32 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.721508026123
2016-08-01 18:47:28.562314: step 4500, loss = 141.66 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.72559595108
2016-08-01 18:47:48.194476: step 4520, loss = 141.80 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72575712204
2016-08-01 18:48:07.865722: step 4540, loss = 138.72 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.723462104797
2016-08-01 18:48:27.550491: step 4560, loss = 145.04 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.726172924042
2016-08-01 18:48:47.205843: step 4580, loss = 142.82 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726869106293
2016-08-01 18:49:06.843099: step 4600, loss = 147.21 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726861000061
2016-08-01 18:49:26.475958: step 4620, loss = 143.99 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728492975235
2016-08-01 18:49:46.046033: step 4640, loss = 143.32 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725075006485
2016-08-01 18:50:05.810559: step 4660, loss = 149.19 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726191043854
2016-08-01 18:50:25.404071: step 4680, loss = 146.39 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724802970886
2016-08-01 18:50:45.065831: step 4700, loss = 146.78 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728458166122
2016-08-01 18:51:04.636686: step 4720, loss = 144.55 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725623846054
2016-08-01 18:51:24.274073: step 4740, loss = 148.86 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727042913437
2016-08-01 18:51:43.854439: step 4760, loss = 147.90 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.73247218132
2016-08-01 18:52:03.517455: step 4780, loss = 148.86 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.726326942444
2016-08-01 18:52:23.207609: step 4800, loss = 150.27 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.73038482666
2016-08-01 18:52:42.893256: step 4820, loss = 153.84 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728357076645
2016-08-01 18:53:02.525416: step 4840, loss = 150.77 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727124929428
2016-08-01 18:53:22.122630: step 4860, loss = 153.07 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727120876312
2016-08-01 18:53:41.752931: step 4880, loss = 151.84 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726418972015
2016-08-01 18:54:01.352490: step 4900, loss = 157.19 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726824998856
2016-08-01 18:54:20.923466: step 4920, loss = 151.16 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726696014404
2016-08-01 18:54:40.613463: step 4940, loss = 151.80 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728857040405
2016-08-01 18:55:00.339177: step 4960, loss = 154.09 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729569196701
2016-08-01 18:55:20.094942: step 4980, loss = 158.80 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730473995209
2016-08-01 18:55:39.767551: step 5000, loss = 157.34 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.723572015762
2016-08-01 18:56:06.507284: step 5020, loss = 155.66 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.72536611557
2016-08-01 18:56:26.074466: step 5040, loss = 156.09 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726412057877
2016-08-01 18:56:45.733149: step 5060, loss = 162.53 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.725777864456
2016-08-01 18:57:05.307629: step 5080, loss = 158.61 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726763010025
2016-08-01 18:57:24.897618: step 5100, loss = 158.64 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.732645988464
2016-08-01 18:57:44.607969: step 5120, loss = 158.78 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.723526954651
2016-08-01 18:58:04.304754: step 5140, loss = 158.79 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.730870008469
2016-08-01 18:58:23.934299: step 5160, loss = 160.37 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.72697687149
2016-08-01 18:58:43.709126: step 5180, loss = 157.08 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729702949524
2016-08-01 18:59:03.353705: step 5200, loss = 162.55 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.719743967056
2016-08-01 18:59:22.919624: step 5220, loss = 161.64 (5.6 examples/sec; 0.720 sec/batch)
num_examples_per_step,duration 4 0.724210977554
2016-08-01 18:59:42.512505: step 5240, loss = 161.98 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.726118803024
2016-08-01 19:00:02.132001: step 5260, loss = 158.79 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727617025375
2016-08-01 19:00:21.878236: step 5280, loss = 160.66 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726146936417
2016-08-01 19:00:41.559045: step 5300, loss = 159.16 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728318929672
2016-08-01 19:01:01.182230: step 5320, loss = 162.15 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72561788559
2016-08-01 19:01:20.864205: step 5340, loss = 163.89 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727077960968
2016-08-01 19:01:40.434112: step 5360, loss = 164.40 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725810050964
2016-08-01 19:02:00.041140: step 5380, loss = 161.93 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727208137512
2016-08-01 19:02:19.705349: step 5400, loss = 163.10 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724390983582
2016-08-01 19:02:39.347027: step 5420, loss = 166.68 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725701093674
2016-08-01 19:02:59.035405: step 5440, loss = 166.02 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.725723981857
2016-08-01 19:03:18.625285: step 5460, loss = 167.48 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728827953339
2016-08-01 19:03:38.305032: step 5480, loss = 165.46 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.723998069763
2016-08-01 19:03:57.991738: step 5500, loss = 167.41 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.724733114243
2016-08-01 19:04:17.662660: step 5520, loss = 164.23 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.726315021515
2016-08-01 19:04:37.305227: step 5540, loss = 168.19 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.725255966187
2016-08-01 19:04:56.951841: step 5560, loss = 170.59 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729624032974
2016-08-01 19:05:16.679283: step 5580, loss = 172.76 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.733946800232
2016-08-01 19:05:36.316050: step 5600, loss = 168.35 (5.4 examples/sec; 0.734 sec/batch)
num_examples_per_step,duration 4 0.728723049164
2016-08-01 19:05:55.905638: step 5620, loss = 171.76 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729793071747
2016-08-01 19:06:15.577094: step 5640, loss = 170.70 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72993016243
2016-08-01 19:06:35.252822: step 5660, loss = 169.47 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729216098785
2016-08-01 19:06:54.838196: step 5680, loss = 170.85 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.730452060699
2016-08-01 19:07:14.647399: step 5700, loss = 170.02 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729205131531
2016-08-01 19:07:34.284484: step 5720, loss = 170.89 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727808952332
2016-08-01 19:07:53.878329: step 5740, loss = 173.50 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726071119308
2016-08-01 19:08:13.525007: step 5760, loss = 173.03 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724000930786
2016-08-01 19:08:33.132215: step 5780, loss = 175.29 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.723968982697
2016-08-01 19:08:52.753937: step 5800, loss = 171.37 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725209951401
2016-08-01 19:09:12.368571: step 5820, loss = 178.56 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.72368311882
2016-08-01 19:09:32.003515: step 5840, loss = 172.11 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.737447023392
2016-08-01 19:09:51.667498: step 5860, loss = 171.81 (5.4 examples/sec; 0.737 sec/batch)
num_examples_per_step,duration 4 0.72863817215
2016-08-01 19:10:11.257082: step 5880, loss = 175.41 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727265119553
2016-08-01 19:10:30.837255: step 5900, loss = 175.43 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724898815155
2016-08-01 19:10:50.400063: step 5920, loss = 175.04 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725982189178
2016-08-01 19:11:10.090153: step 5940, loss = 174.43 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72692990303
2016-08-01 19:11:29.649861: step 5960, loss = 179.46 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724875211716
2016-08-01 19:11:49.364136: step 5980, loss = 173.35 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724996805191
2016-08-01 19:12:08.954523: step 6000, loss = 175.42 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724652051926
2016-08-01 19:12:40.303060: step 6020, loss = 177.59 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727008104324
2016-08-01 19:12:59.880730: step 6040, loss = 175.41 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728840827942
2016-08-01 19:13:19.470134: step 6060, loss = 176.79 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72464299202
2016-08-01 19:13:39.052702: step 6080, loss = 179.34 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727293014526
2016-08-01 19:13:58.742164: step 6100, loss = 175.07 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724452972412
2016-08-01 19:14:18.408180: step 6120, loss = 177.75 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.728416919708
2016-08-01 19:14:38.054737: step 6140, loss = 187.79 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727780103683
2016-08-01 19:14:57.736984: step 6160, loss = 175.18 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725730895996
2016-08-01 19:15:17.468163: step 6180, loss = 176.22 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727566957474
2016-08-01 19:15:37.058450: step 6200, loss = 177.10 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728898048401
2016-08-01 19:15:56.587727: step 6220, loss = 180.19 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727614879608
2016-08-01 19:16:16.110547: step 6240, loss = 191.28 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728924036026
2016-08-01 19:16:35.805143: step 6260, loss = 184.76 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728011846542
2016-08-01 19:16:55.551419: step 6280, loss = 179.25 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729907989502
2016-08-01 19:17:15.155625: step 6300, loss = 183.31 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728854894638
2016-08-01 19:17:34.802677: step 6320, loss = 181.40 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727760076523
2016-08-01 19:17:54.460203: step 6340, loss = 180.66 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729257106781
2016-08-01 19:18:14.065024: step 6360, loss = 185.31 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.732069969177
2016-08-01 19:18:33.737715: step 6380, loss = 181.02 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.725615978241
2016-08-01 19:18:53.302938: step 6400, loss = 181.37 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72662115097
2016-08-01 19:19:12.933417: step 6420, loss = 185.31 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726614952087
2016-08-01 19:19:32.598660: step 6440, loss = 188.24 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724602937698
2016-08-01 19:19:52.166908: step 6460, loss = 184.69 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727582931519
2016-08-01 19:20:11.799542: step 6480, loss = 182.67 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.726901054382
2016-08-01 19:20:31.445340: step 6500, loss = 183.65 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726224899292
2016-08-01 19:20:51.036325: step 6520, loss = 185.09 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727347135544
2016-08-01 19:21:10.604646: step 6540, loss = 187.78 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72580909729
2016-08-01 19:21:30.228909: step 6560, loss = 185.28 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72921705246
2016-08-01 19:21:49.896756: step 6580, loss = 183.43 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727396011353
2016-08-01 19:22:09.442025: step 6600, loss = 186.24 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728698015213
2016-08-01 19:22:29.110997: step 6620, loss = 191.40 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726830005646
2016-08-01 19:22:48.732641: step 6640, loss = 190.95 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725709199905
2016-08-01 19:23:08.594399: step 6660, loss = 185.90 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727648019791
2016-08-01 19:23:28.348645: step 6680, loss = 190.78 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729157924652
2016-08-01 19:23:47.933217: step 6700, loss = 187.36 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725488901138
2016-08-01 19:24:07.537644: step 6720, loss = 192.75 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729104995728
2016-08-01 19:24:27.083886: step 6740, loss = 198.21 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72828912735
2016-08-01 19:24:46.815566: step 6760, loss = 189.63 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728672027588
2016-08-01 19:25:06.474540: step 6780, loss = 186.19 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729470968246
2016-08-01 19:25:26.089453: step 6800, loss = 187.30 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725573062897
2016-08-01 19:25:45.714928: step 6820, loss = 189.26 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.723173856735
2016-08-01 19:26:05.339521: step 6840, loss = 189.77 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.724194049835
2016-08-01 19:26:25.129288: step 6860, loss = 193.31 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.721876144409
2016-08-01 19:26:44.784609: step 6880, loss = 193.16 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.722672939301
2016-08-01 19:27:04.365110: step 6900, loss = 186.21 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.729757070541
2016-08-01 19:27:23.939397: step 6920, loss = 190.56 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.728230953217
2016-08-01 19:27:43.522097: step 6940, loss = 189.48 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727194070816
2016-08-01 19:28:03.173754: step 6960, loss = 188.87 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725572109222
2016-08-01 19:28:22.836608: step 6980, loss = 188.27 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724451065063
2016-08-01 19:28:42.547435: step 7000, loss = 187.32 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.724740982056
2016-08-01 19:29:08.897980: step 7020, loss = 191.39 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.731575012207
2016-08-01 19:29:28.539082: step 7040, loss = 189.23 (5.5 examples/sec; 0.732 sec/batch)
num_examples_per_step,duration 4 0.725882053375
2016-08-01 19:29:48.136942: step 7060, loss = 191.66 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72669005394
2016-08-01 19:30:07.729796: step 7080, loss = 189.81 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725482940674
2016-08-01 19:30:27.354824: step 7100, loss = 189.49 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727753162384
2016-08-01 19:30:46.962404: step 7120, loss = 190.49 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72801399231
2016-08-01 19:31:06.602395: step 7140, loss = 187.66 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.7262840271
2016-08-01 19:31:26.186875: step 7160, loss = 200.64 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727246046066
2016-08-01 19:31:45.710367: step 7180, loss = 193.23 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727802038193
2016-08-01 19:32:05.391276: step 7200, loss = 199.70 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.730815887451
2016-08-01 19:32:24.954478: step 7220, loss = 191.66 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.723829984665
2016-08-01 19:32:44.683332: step 7240, loss = 190.76 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.727766036987
2016-08-01 19:33:04.357147: step 7260, loss = 190.34 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727823019028
2016-08-01 19:33:23.947606: step 7280, loss = 198.95 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728528022766
2016-08-01 19:33:43.611852: step 7300, loss = 190.95 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728180885315
2016-08-01 19:34:03.243239: step 7320, loss = 192.14 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727752923965
2016-08-01 19:34:23.042406: step 7340, loss = 195.14 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.731307029724
2016-08-01 19:34:42.813979: step 7360, loss = 189.16 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.723731040955
2016-08-01 19:35:02.512923: step 7380, loss = 195.48 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.727295875549
2016-08-01 19:35:22.083077: step 7400, loss = 190.55 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.723644971848
2016-08-01 19:35:41.695754: step 7420, loss = 202.32 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725770950317
2016-08-01 19:36:01.235494: step 7440, loss = 196.23 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.723998069763
2016-08-01 19:36:20.868725: step 7460, loss = 192.54 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.729373931885
2016-08-01 19:36:40.545302: step 7480, loss = 191.53 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728363990784
2016-08-01 19:37:00.156030: step 7500, loss = 188.10 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72870516777
2016-08-01 19:37:19.767455: step 7520, loss = 209.11 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725201845169
2016-08-01 19:37:39.381986: step 7540, loss = 197.61 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.724174976349
2016-08-01 19:37:59.022297: step 7560, loss = 193.04 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.725744962692
2016-08-01 19:38:18.617462: step 7580, loss = 187.79 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.720616102219
2016-08-01 19:38:38.272689: step 7600, loss = 188.35 (5.6 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.723273992538
2016-08-01 19:38:57.922185: step 7620, loss = 191.94 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.728180885315
2016-08-01 19:39:17.528091: step 7640, loss = 189.25 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.730345010757
2016-08-01 19:39:37.144926: step 7660, loss = 190.25 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.731482028961
2016-08-01 19:39:56.732092: step 7680, loss = 186.59 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.726866960526
2016-08-01 19:40:17.123787: step 7700, loss = 191.02 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725457906723
2016-08-01 19:40:36.771778: step 7720, loss = 186.26 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725181102753
2016-08-01 19:40:56.397393: step 7740, loss = 188.88 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.723675012589
2016-08-01 19:41:16.042742: step 7760, loss = 188.70 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.729047060013
2016-08-01 19:41:35.676620: step 7780, loss = 190.20 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725350141525
2016-08-01 19:41:55.486680: step 7800, loss = 187.36 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729562044144
2016-08-01 19:42:15.115803: step 7820, loss = 186.59 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730222940445
2016-08-01 19:42:36.533067: step 7840, loss = 187.64 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72806596756
2016-08-01 19:42:56.300085: step 7860, loss = 188.46 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729342937469
2016-08-01 19:43:15.950206: step 7880, loss = 185.80 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72921705246
2016-08-01 19:43:35.655825: step 7900, loss = 190.07 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726514816284
2016-08-01 19:43:55.269161: step 7920, loss = 186.78 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729511022568
2016-08-01 19:44:14.930359: step 7940, loss = 185.29 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.727902889252
2016-08-01 19:44:34.641934: step 7960, loss = 183.21 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.723570108414
2016-08-01 19:44:54.542402: step 7980, loss = 187.04 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.729953050613
2016-08-01 19:45:14.218488: step 8000, loss = 189.71 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726665973663
2016-08-01 19:45:41.157974: step 8020, loss = 182.39 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729791164398
2016-08-01 19:46:00.805333: step 8040, loss = 183.86 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730164051056
2016-08-01 19:46:20.427942: step 8060, loss = 188.21 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.725610971451
2016-08-01 19:46:43.318316: step 8080, loss = 187.89 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726763010025
2016-08-01 19:47:03.019966: step 8100, loss = 183.55 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726567983627
2016-08-01 19:47:22.627800: step 8120, loss = 186.26 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.723416805267
2016-08-01 19:47:42.254404: step 8140, loss = 183.33 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.728416919708
2016-08-01 19:48:01.883138: step 8160, loss = 183.30 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725466966629
2016-08-01 19:48:21.522212: step 8180, loss = 182.23 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725587844849
2016-08-01 19:48:41.153997: step 8200, loss = 182.05 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726965904236
2016-08-01 19:49:00.785191: step 8220, loss = 187.64 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730365037918
2016-08-01 19:49:20.385844: step 8240, loss = 186.40 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.73015999794
2016-08-01 19:49:40.015445: step 8260, loss = 184.17 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.72714805603
2016-08-01 19:49:59.732660: step 8280, loss = 181.32 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.7297539711
2016-08-01 19:50:19.421980: step 8300, loss = 183.40 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.727826833725
2016-08-01 19:50:39.032645: step 8320, loss = 183.43 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728219985962
2016-08-01 19:50:58.636174: step 8340, loss = 183.50 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728735208511
2016-08-01 19:51:18.321572: step 8360, loss = 183.11 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726126909256
2016-08-01 19:51:38.031652: step 8380, loss = 186.01 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728428840637
2016-08-01 19:51:57.660681: step 8400, loss = 185.60 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727374076843
2016-08-01 19:52:17.312243: step 8420, loss = 179.97 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728182077408
2016-08-01 19:52:36.909135: step 8440, loss = 184.40 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729966163635
2016-08-01 19:52:56.583559: step 8460, loss = 180.13 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.729537010193
2016-08-01 19:53:16.282683: step 8480, loss = 181.97 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.730809926987
2016-08-01 19:53:35.968024: step 8500, loss = 192.19 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.726459980011
2016-08-01 19:53:55.647714: step 8520, loss = 180.62 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.728525161743
2016-08-01 19:54:15.333080: step 8540, loss = 182.09 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727553129196
2016-08-01 19:54:34.937880: step 8560, loss = 184.31 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.730808019638
2016-08-01 19:54:54.730628: step 8580, loss = 180.38 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.726816177368
2016-08-01 19:55:14.409660: step 8600, loss = 182.11 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727504014969
2016-08-01 19:55:34.111577: step 8620, loss = 177.35 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729429960251
2016-08-01 19:55:53.767103: step 8640, loss = 176.79 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.726805925369
2016-08-01 19:56:13.453929: step 8660, loss = 183.22 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726535081863
2016-08-01 19:56:33.162547: step 8680, loss = 184.56 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724622964859
2016-08-01 19:56:52.862446: step 8700, loss = 179.70 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728453874588
2016-08-01 19:57:12.612925: step 8720, loss = 182.78 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.723857164383
2016-08-01 19:57:32.338561: step 8740, loss = 183.55 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.723706007004
2016-08-01 19:57:52.072479: step 8760, loss = 185.77 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.724961042404
2016-08-01 19:58:11.830341: step 8780, loss = 180.11 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.723494052887
2016-08-01 19:58:31.515232: step 8800, loss = 180.54 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.723182201385
2016-08-01 19:58:51.213766: step 8820, loss = 180.12 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.724902153015
2016-08-01 19:59:10.821557: step 8840, loss = 182.59 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.723664045334
2016-08-01 19:59:30.506192: step 8860, loss = 177.24 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.728952884674
2016-08-01 19:59:50.195945: step 8880, loss = 183.49 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.727375030518
2016-08-01 20:00:09.987085: step 8900, loss = 181.52 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.728525876999
2016-08-01 20:00:29.638685: step 8920, loss = 178.94 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725353956223
2016-08-01 20:00:49.398603: step 8940, loss = 177.27 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.729129076004
2016-08-01 20:01:09.129653: step 8960, loss = 188.16 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728667020798
2016-08-01 20:01:28.922681: step 8980, loss = 179.15 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728091955185
2016-08-01 20:01:48.634917: step 9000, loss = 179.19 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727974891663
2016-08-01 20:02:15.518255: step 9020, loss = 177.45 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.72500705719
2016-08-01 20:02:35.266677: step 9040, loss = 184.94 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727174043655
2016-08-01 20:02:54.950237: step 9060, loss = 177.72 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.724570035934
2016-08-01 20:03:14.596074: step 9080, loss = 181.29 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.731477022171
2016-08-01 20:03:34.308682: step 9100, loss = 178.78 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.72532582283
2016-08-01 20:03:53.962614: step 9120, loss = 180.17 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.725265979767
2016-08-01 20:04:13.651479: step 9140, loss = 181.66 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727149009705
2016-08-01 20:04:33.315608: step 9160, loss = 177.04 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.727780103683
2016-08-01 20:04:53.045536: step 9180, loss = 174.93 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.729434013367
2016-08-01 20:05:12.692441: step 9200, loss = 176.28 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.72635102272
2016-08-01 20:05:32.432615: step 9220, loss = 175.22 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.729072093964
2016-08-01 20:05:52.119478: step 9240, loss = 180.13 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725611925125
2016-08-01 20:06:11.823742: step 9260, loss = 177.57 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.72562623024
2016-08-01 20:06:31.492720: step 9280, loss = 180.53 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727875947952
2016-08-01 20:06:51.219094: step 9300, loss = 176.70 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.725071907043
2016-08-01 20:07:10.867019: step 9320, loss = 177.12 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.728941917419
2016-08-01 20:07:30.592289: step 9340, loss = 176.90 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.725600004196
2016-08-01 20:07:50.188830: step 9360, loss = 179.96 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.727318048477
2016-08-01 20:08:09.881580: step 9380, loss = 180.43 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.725780010223
2016-08-01 20:08:29.629781: step 9400, loss = 178.60 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.726342201233
2016-08-01 20:08:49.280250: step 9420, loss = 174.54 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.724560022354
2016-08-01 20:09:08.896194: step 9440, loss = 177.75 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.727175951004
2016-08-01 20:09:28.529544: step 9460, loss = 182.45 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.729142904282
2016-08-01 20:09:48.424073: step 9480, loss = 177.10 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.728303909302
2016-08-01 20:10:08.260617: step 9500, loss = 173.46 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.728885173798
2016-08-01 20:10:27.947342: step 9520, loss = 174.85 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729187965393
2016-08-01 20:10:47.629954: step 9540, loss = 186.91 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.731200933456
2016-08-01 20:11:07.330538: step 9560, loss = 178.72 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728479146957
2016-08-01 20:11:26.942257: step 9580, loss = 171.98 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.724275112152
2016-08-01 20:11:46.632750: step 9600, loss = 174.06 (5.5 examples/sec; 0.724 sec/batch)
num_examples_per_step,duration 4 0.732805013657
2016-08-01 20:12:06.367677: step 9620, loss = 176.34 (5.5 examples/sec; 0.733 sec/batch)
num_examples_per_step,duration 4 0.726811170578
2016-08-01 20:12:27.921996: step 9640, loss = 177.54 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.726422071457
2016-08-01 20:12:47.600472: step 9660, loss = 173.70 (5.5 examples/sec; 0.726 sec/batch)
num_examples_per_step,duration 4 0.731148958206
2016-08-01 20:13:07.324541: step 9680, loss = 174.03 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.730725049973
2016-08-01 20:13:26.982410: step 9700, loss = 171.24 (5.5 examples/sec; 0.731 sec/batch)
num_examples_per_step,duration 4 0.728499889374
2016-08-01 20:13:46.732894: step 9720, loss = 179.88 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.727225065231
2016-08-01 20:14:06.554425: step 9740, loss = 174.25 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.72865486145
2016-08-01 20:14:26.303186: step 9760, loss = 176.41 (5.5 examples/sec; 0.729 sec/batch)
num_examples_per_step,duration 4 0.729959011078
2016-08-01 20:14:46.011303: step 9780, loss = 176.38 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.726670026779
2016-08-01 20:15:05.638650: step 9800, loss = 175.87 (5.5 examples/sec; 0.727 sec/batch)
num_examples_per_step,duration 4 0.730032920837
2016-08-01 20:15:25.275678: step 9820, loss = 176.52 (5.5 examples/sec; 0.730 sec/batch)
num_examples_per_step,duration 4 0.724806070328
2016-08-01 20:15:45.009814: step 9840, loss = 175.71 (5.5 examples/sec; 0.725 sec/batch)
num_examples_per_step,duration 4 0.722449064255
2016-08-01 20:16:04.690144: step 9860, loss = 174.50 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.728430986404
2016-08-01 20:16:24.495858: step 9880, loss = 171.92 (5.5 examples/sec; 0.728 sec/batch)
num_examples_per_step,duration 4 0.722286939621
2016-08-01 20:16:44.152997: step 9900, loss = 177.90 (5.5 examples/sec; 0.722 sec/batch)
num_examples_per_step,duration 4 0.721238851547
2016-08-01 20:17:03.760306: step 9920, loss = 172.56 (5.5 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.723480939865
2016-08-01 20:17:23.503527: step 9940, loss = 178.25 (5.5 examples/sec; 0.723 sec/batch)
num_examples_per_step,duration 4 0.721353054047
2016-08-01 20:17:43.298012: step 9960, loss = 177.40 (5.5 examples/sec; 0.721 sec/batch)
num_examples_per_step,duration 4 0.72557592392
2016-08-01 20:18:02.999633: step 9980, loss = 175.03 (5.5 examples/sec; 0.726 sec/batch)
+ date
Mon Aug  1 20:18:29 EDT 2016
